{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLAB = True\n",
    "if COLLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = \"/content/drive/MyDrive/Kodiak/STORAGE/fake_slo/masks\"\n",
    "else:\n",
    "    data_path = \"masks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "for image_name in sorted(os.listdir(data_path)):\n",
    "    image_path = os.path.join(data_path, image_name)\n",
    "    if not os.path.isfile(image_path):\n",
    "        continue\n",
    "    if not image_name.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\")):\n",
    "        continue\n",
    "    print(image_name)\n",
    "    img = Image.open(image_path)\n",
    "    # display(img)  # shows inline in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af614d",
   "metadata": {},
   "source": [
    "## TensorFlow DDPM For Mask Synthesis\n",
    "Train a diffusion model on your vessel masks and generate new synthetic binary masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tf_ddpm_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow DDPM (Mask Synthesis)\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))\n",
    "# Optional but useful in Colab to avoid full GPU pre-allocation.\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(data_path) if \"data_path\" in globals() else Path(\"masks\")\n",
    "MODEL_DIR = DATA_DIR.parent / \"tf_ddpm_mask_model\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "DIFFUSION_STEPS = 1000\n",
    "THRESHOLD = 0  # black stays 0, anything > 0 becomes vessel/white\n",
    "\n",
    "image_files = sorted(\n",
    "    str(p)\n",
    "    for p in DATA_DIR.rglob(\"*\")\n",
    "    if p.is_file() and p.suffix.lower() in {\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\"}\n",
    ")\n",
    "print(f\"Found {len(image_files)} training images in {DATA_DIR}\")\n",
    "if not image_files:\n",
    "    raise ValueError(f\"No training images found in {DATA_DIR}\")\n",
    "\n",
    "\n",
    "def _load_mask_py(path_tensor):\n",
    "    path = path_tensor.numpy().decode(\"utf-8\")\n",
    "    with Image.open(path) as img:\n",
    "        img = img.convert(\"L\")\n",
    "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.Resampling.NEAREST)\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "\n",
    "    arr = (arr > THRESHOLD).astype(np.float32)\n",
    "    arr = arr[..., None]  # HWC with one channel\n",
    "    arr = arr * 2.0 - 1.0  # scale to [-1, 1]\n",
    "    return arr\n",
    "\n",
    "\n",
    "def load_mask_tf(path_tensor):\n",
    "    x = tf.py_function(_load_mask_py, [path_tensor], Tout=tf.float32)\n",
    "    x.set_shape((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(image_files)\n",
    "    .shuffle(len(image_files), seed=SEED, reshuffle_each_iteration=True)\n",
    "    .map(load_mask_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(timesteps, dim=128):\n",
    "    half = dim // 2\n",
    "    freqs = tf.exp(-math.log(10000.0) * tf.range(half, dtype=tf.float32) / float(max(half - 1, 1)))\n",
    "    args = tf.cast(tf.expand_dims(timesteps, 1), tf.float32) * tf.expand_dims(freqs, 0)\n",
    "    emb = tf.concat([tf.sin(args), tf.cos(args)], axis=-1)\n",
    "    if dim % 2 == 1:\n",
    "        emb = tf.pad(emb, [[0, 0], [0, 1]])\n",
    "    return emb\n",
    "\n",
    "\n",
    "def residual_block(x, t_emb, channels, name):\n",
    "    in_channels = x.shape[-1]\n",
    "\n",
    "    h = tf.keras.layers.Conv2D(channels, 3, padding=\"same\", name=f\"{name}_conv1\")(x)\n",
    "    h = tf.keras.layers.BatchNormalization(name=f\"{name}_bn1\")(h)\n",
    "    h = tf.keras.layers.Activation(\"swish\", name=f\"{name}_act1\")(h)\n",
    "\n",
    "    t_proj = tf.keras.layers.Dense(channels, activation=\"swish\", name=f\"{name}_tproj\")(t_emb)\n",
    "    t_proj = tf.keras.layers.Reshape((1, 1, channels), name=f\"{name}_treshape\")(t_proj)\n",
    "    h = tf.keras.layers.Add(name=f\"{name}_tadd\")([h, t_proj])\n",
    "\n",
    "    h = tf.keras.layers.Conv2D(channels, 3, padding=\"same\", name=f\"{name}_conv2\")(h)\n",
    "    h = tf.keras.layers.BatchNormalization(name=f\"{name}_bn2\")(h)\n",
    "    h = tf.keras.layers.Activation(\"swish\", name=f\"{name}_act2\")(h)\n",
    "\n",
    "    if in_channels != channels:\n",
    "        x = tf.keras.layers.Conv2D(channels, 1, padding=\"same\", name=f\"{name}_skip\")(x)\n",
    "\n",
    "    return tf.keras.layers.Add(name=f\"{name}_out\")([x, h])\n",
    "\n",
    "\n",
    "def build_unet(image_size):\n",
    "    image_in = tf.keras.Input(shape=(image_size, image_size, 1), name=\"image\")\n",
    "    t_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"timestep\")\n",
    "\n",
    "    t_emb = tf.keras.layers.Lambda(lambda t: sinusoidal_embedding(t, 128), name=\"time_sin\")(t_in)\n",
    "    t_emb = tf.keras.layers.Dense(256, activation=\"swish\", name=\"time_dense1\")(t_emb)\n",
    "    t_emb = tf.keras.layers.Dense(256, activation=\"swish\", name=\"time_dense2\")(t_emb)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", name=\"in_conv\")(image_in)\n",
    "\n",
    "    s1 = residual_block(x, t_emb, 64, \"down1\")\n",
    "    x = tf.keras.layers.Conv2D(64, 3, strides=2, padding=\"same\", name=\"downsample1\")(s1)\n",
    "\n",
    "    s2 = residual_block(x, t_emb, 128, \"down2\")\n",
    "    x = tf.keras.layers.Conv2D(128, 3, strides=2, padding=\"same\", name=\"downsample2\")(s2)\n",
    "\n",
    "    x = residual_block(x, t_emb, 256, \"mid1\")\n",
    "    x = residual_block(x, t_emb, 256, \"mid2\")\n",
    "\n",
    "    x = tf.keras.layers.UpSampling2D(size=2, interpolation=\"nearest\", name=\"upsample1\")(x)\n",
    "    x = tf.keras.layers.Concatenate(name=\"concat1\")([x, s2])\n",
    "    x = residual_block(x, t_emb, 128, \"up1\")\n",
    "\n",
    "    x = tf.keras.layers.UpSampling2D(size=2, interpolation=\"nearest\", name=\"upsample2\")(x)\n",
    "    x = tf.keras.layers.Concatenate(name=\"concat2\")([x, s1])\n",
    "    x = residual_block(x, t_emb, 64, \"up2\")\n",
    "\n",
    "    out = tf.keras.layers.Conv2D(1, 1, padding=\"same\", name=\"noise_pred\")(x)\n",
    "    return tf.keras.Model([image_in, t_in], out, name=\"mask_ddpm_unet\")\n",
    "\n",
    "\n",
    "model = build_unet(IMAGE_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "betas = np.linspace(1e-4, 2e-2, DIFFUSION_STEPS, dtype=np.float32)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = np.cumprod(alphas, axis=0).astype(np.float32)\n",
    "\n",
    "betas_tf = tf.constant(betas)\n",
    "alphas_tf = tf.constant(alphas)\n",
    "alpha_bars_tf = tf.constant(alpha_bars)\n",
    "\n",
    "\n",
    "def q_sample(x0, t, noise):\n",
    "    a_bar = tf.gather(alpha_bars_tf, t)\n",
    "    a_bar = tf.reshape(a_bar, (-1, 1, 1, 1))\n",
    "    return tf.sqrt(a_bar) * x0 + tf.sqrt(1.0 - a_bar) * noise\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x0):\n",
    "    b = tf.shape(x0)[0]\n",
    "    t = tf.random.uniform((b,), minval=0, maxval=DIFFUSION_STEPS, dtype=tf.int32)\n",
    "    noise = tf.random.normal(tf.shape(x0))\n",
    "    x_t = q_sample(x0, t, noise)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_noise = model([x_t, t], training=True)\n",
    "        loss = tf.reduce_mean(tf.square(noise - pred_noise))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    losses = []\n",
    "    for batch in dataset:\n",
    "        loss = train_step(batch)\n",
    "        losses.append(float(loss.numpy()))\n",
    "\n",
    "    mean_loss = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "    print(f\"Epoch {epoch:03d}/{EPOCHS} | loss={mean_loss:.6f}\")\n",
    "\n",
    "\n",
    "weights_path = MODEL_DIR / \"model.weights.h5\"\n",
    "model.save_weights(weights_path)\n",
    "\n",
    "np.savez(\n",
    "    MODEL_DIR / \"schedule.npz\",\n",
    "    betas=betas,\n",
    "    alphas=alphas,\n",
    "    alpha_bars=alpha_bars,\n",
    ")\n",
    "\n",
    "(MODEL_DIR / \"config.json\").write_text(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"image_size\": IMAGE_SIZE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"diffusion_steps\": DIFFUSION_STEPS,\n",
    "            \"threshold\": THRESHOLD,\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "        },\n",
    "        indent=2,\n",
    "    ),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Saved model to\", MODEL_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tf_ddpm_sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic masks with trained TensorFlow DDPM\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "if \"build_unet\" not in globals():\n",
    "    raise RuntimeError(\"Run the TensorFlow DDPM training cell first (it defines build_unet).\")\n",
    "\n",
    "MODEL_DIR = Path(data_path).parent / \"tf_ddpm_mask_model\" if \"data_path\" in globals() else Path(\"tf_ddpm_mask_model\")\n",
    "OUT_DIR = MODEL_DIR.parent / \"generated_masks_tf\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg_path = MODEL_DIR / \"config.json\"\n",
    "if cfg_path.exists():\n",
    "    cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "    IMAGE_SIZE = int(cfg.get(\"image_size\", globals().get(\"IMAGE_SIZE\", 256)))\n",
    "    DIFFUSION_STEPS = int(cfg.get(\"diffusion_steps\", globals().get(\"DIFFUSION_STEPS\", 1000)))\n",
    "else:\n",
    "    IMAGE_SIZE = int(globals().get(\"IMAGE_SIZE\", 256))\n",
    "    DIFFUSION_STEPS = int(globals().get(\"DIFFUSION_STEPS\", 1000))\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "BATCH_SIZE = 8\n",
    "INFERENCE_STEPS = DIFFUSION_STEPS\n",
    "BINARY_THRESHOLD = 0.5\n",
    "\n",
    "# Rebuild model and load weights/schedule.\n",
    "model = build_unet(IMAGE_SIZE)\n",
    "model.load_weights(MODEL_DIR / \"model.weights.h5\")\n",
    "\n",
    "sched = np.load(MODEL_DIR / \"schedule.npz\")\n",
    "betas = sched[\"betas\"].astype(np.float32)\n",
    "alphas = sched[\"alphas\"].astype(np.float32)\n",
    "alpha_bars = sched[\"alpha_bars\"].astype(np.float32)\n",
    "\n",
    "\n",
    "def p_sample_loop(num_samples, batch_size, inference_steps):\n",
    "    saved = 0\n",
    "    sample_idx = 0\n",
    "\n",
    "    while saved < num_samples:\n",
    "        current_batch = min(batch_size, num_samples - saved)\n",
    "        x = tf.random.normal((current_batch, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "        for t in reversed(range(inference_steps)):\n",
    "            t_batch = tf.fill([current_batch], t)\n",
    "            eps_theta = model([x, t_batch], training=False)\n",
    "\n",
    "            alpha_t = alphas[t]\n",
    "            alpha_bar_t = alpha_bars[t]\n",
    "            beta_t = betas[t]\n",
    "\n",
    "            coef1 = 1.0 / np.sqrt(alpha_t)\n",
    "            coef2 = (1.0 - alpha_t) / np.sqrt(1.0 - alpha_bar_t)\n",
    "\n",
    "            if t > 0:\n",
    "                z = tf.random.normal(tf.shape(x))\n",
    "            else:\n",
    "                z = tf.zeros_like(x)\n",
    "\n",
    "            x = coef1 * (x - coef2 * eps_theta) + np.sqrt(beta_t) * z\n",
    "\n",
    "        x_np = x.numpy()\n",
    "        x_np = (x_np + 1.0) / 2.0\n",
    "        x_np = np.clip(x_np, 0.0, 1.0)\n",
    "\n",
    "        for i in range(current_batch):\n",
    "            gray = x_np[i, ..., 0]\n",
    "            binary = (gray > BINARY_THRESHOLD).astype(np.uint8) * 255\n",
    "            out_path = OUT_DIR / f\"mask_{sample_idx:05d}.tiff\"\n",
    "            Image.fromarray(binary, mode=\"L\").save(out_path, format=\"TIFF\")\n",
    "            sample_idx += 1\n",
    "            saved += 1\n",
    "\n",
    "            if saved >= num_samples:\n",
    "                break\n",
    "\n",
    "    return sample_idx\n",
    "\n",
    "\n",
    "count = p_sample_loop(NUM_SAMPLES, BATCH_SIZE, INFERENCE_STEPS)\n",
    "print(f\"Saved {count} synthetic masks to {OUT_DIR}\")\n",
    "\n",
    "# Quick preview of first 6 generated masks.\n",
    "preview = sorted(OUT_DIR.glob(\"mask_*.tiff\"))[:6]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for ax, p in zip(axes.flatten(), preview):\n",
    "    with Image.open(p) as img:\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(p.name)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
